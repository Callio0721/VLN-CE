python run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma_pm_da_aug_tune.yaml \
    --run-type train

python run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma.yaml \
    --run-type train

tensorboard --logdir /data/DataLACP/zhangbodong/ZBD/VLN-CE/data/tensorboard_dirs --port=6008
tensorboard --logdir /home/ShiKaituo/ZhangBodong/VLN-CE/data/tensorboard_dirs
<<<<<<< HEAD
=======
tensorboard --logdir /data2/zhangbodong/VLN-CE/data/tensorboard_dirs

>>>>>>> 1efb25eb0a2e7c62782faafba2cd0632b7dc8630
查看当前目录中文件的修改日期（mtime）
ls -lt

查看当前目录总大小
du -sh .

当前目录下每个子目录/文件的大小
du -sh *

# 记得换下面的yaml文件
python run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma_clip_pm_aug.yaml \ 
    --run-type train \
    SIMULATOR_GPU_IDS "[3]" \
    NUM_ENVIRONMENTS 4 \
    IL.DAGGER.update_size 100 \
    IL.epochs 2 \
    IL.batch_size 2 \
    TENSORBOARD_DIR data/tensorboard_dirs/debug_test \
    CHECKPOINT_FOLDER data/checkpoints/debug_test \
    IL.DAGGER.lmdb_commit_frequency 1\
    lmdb_features_dir data/trajectories_dirs/debug_test/trajectories.lmdb

复制某个终端的输出到文件

在运行训练之前执行：
script -f /data/DataLACP/zhangbodong/ZBD/VLN-CE/log/training_log.txt

script -f /home/ShiKaituo/ZhangBodong/VLN-CE/data/log/eval.log
script -f /home/ShiKaituo/ZhangBodong/VLN-CE/data/checkpoints/candidate_cma_clip_bert/candidate_cma_clip_bert.log
script -f /data2/zhangbodong/VLN-CE/data/log/train_clip_bert_log.log
然后正常运行你的深度学习代码

关闭scrip的方式：
1、在那个正在运行 script 的终端里直接输入：exit # 停止 script 记录，终端继续可用，不会关闭
2、直接按 Ctrl+D 也可以退出 script 会话。 # 终端不会关闭
3、不确定是否还在 script 中，可以查看提示符或执行： echo $SCRIPT

如果训练已经在跑了（没有经过测试）
# 1. 找到训练进程的PID
ps aux | grep python

# 2. 在另一个终端执行
sudo strace -e trace=write -p <PID> -s 9999 2>&1 | \
  awk '/^write\(1,/{gsub(/^write\([^"]*"|"[^"]*$/, ""); print}' >> /data/DataLACP/zhangbodong/ZBD/VLN-CE/log/training_log.txt &



开启tmux终端运行代码：

第一步：进入 Tmux
tmux new -s resume_train

退出Tmux ctrl+b 再按D
恢复Tmux tmux attach

第二步：启动录制（使用追加模式或新文件）
# 建议用 -a 追加，或者换个文件名
script -f  /home/ShiKaituo/ZhangBodong/VLN-CE/data/log/training_clip_bert_log.txt
script -f  /home/ShiKaituo/ZhangBodong/VLN-CE/data/checkpoints/cma_clip_pm_da_aug_tune/second.log
script -f  /home/ShiKaituo/ZhangBodong/VLN-CE/data/checkpoints/cma_clip_pm_da_aug_tune/evals/eval.log
script -f /data2/zhangbodong/VLN-CE/data/log/train_clip_bert_log.log
script -f /data2/zhangbodong/VLN-CE/data/checkpoints/cma_clip_bert_pm_aug/evals/eval.log
script -f /data2/zhangbodong/VLN-CE/data/log/train_clip_bert_pm_da_aug_tune.log
第三步：运行 Python 训练代码

conda activate vln-ce

# 我为你换到了 29505 端口，这通常能避开冲突
torchrun --nproc_per_node=4 --master_port=29505 run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma_clip_bert_pm_aug.yaml \
    --run-type train \
    IL.load_from_ckpt True \
    IL.ckpt_to_load "data/checkpoints/cma_clip_pm_aug/ckpt.20.pth" \
    IL.is_requeue True \
    SIMULATOR_GPU_IDS "[0,1,2,3]"




# 运行收集命令 (注意这里用 python 而不是 torchrun)
python run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma_clip_bert_pm_aug.yaml \
    --run-type train \
    SIMULATOR_GPU_IDS "[0, 1, 2, 3]" \
    TORCH_GPU_ID 0 \
    NUM_ENVIRONMENTS 12 \
    IL.batch_size 1 \
    IL.DAGGER.p 1.0 \
    IL.DAGGER.preload_lmdb_features False \
    IL.DAGGER.lmdb_commit_frequency 500


# 运行训练命令 (使用 torchrun 启动 4 卡 DDP)
torchrun --nproc_per_node=4 run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma_clip_bert_pm_aug.yaml \
    --run-type train \
    SIMULATOR_GPU_IDS "[0, 1, 2, 3]" \
    NUM_ENVIRONMENTS 6 \
    IL.batch_size 5 \
    IL.DAGGER.preload_lmdb_features False



python run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma_clip_pm_da_aug_tune.yaml \
    --run-type eval \
python run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma_clip_bert_pm_aug.yaml \
    --run-type eval \


torchrun --nproc_per_node=8 run.py \
    --exp-config vlnce_baselines/config/r2r_baselines/cma_clip_bert_pm_da_aug_tune.yaml \
    --run-type train \

import torch
from torch.utils.data import Dataset, DataLoader
import lmdb
import msgpack_numpy
import numpy as np
import tqdm
import os

# ---------------- é…ç½®åŒºåŸŸ ----------------
lmdb_path = "/home/ShiKaituo/ZhangBodong/VLN-CE/data/trajectories_dirs/cma_clip_pm_da_aug_tune/trajectories.lmdb"
vocab_size_limit = 2502
# ----------------------------------------

class VocabCheckDataset(Dataset):
    def __init__(self, lmdb_path):
        self.lmdb_path = lmdb_path
        print(f"ğŸ“– [é˜¶æ®µ1/2] æ­£åœ¨åˆå§‹åŒ– Dataset...")
        
        self.env = lmdb.open(lmdb_path, readonly=True, lock=False)
        with self.env.begin() as txn:
            self.length = txn.stat()['entries']
            # ğŸ”¥ ä¼˜åŒ–ï¼šç»™ Key çš„åŠ è½½è¿‡ç¨‹ä¹ŸåŠ ä¸Šè¿›åº¦æ¡ï¼Œè¿™æ ·ä½ å°±çŸ¥é“æ²¡å¡æ­»
            with txn.cursor() as curs:
                self.keys = []
                # è¿™é‡Œçš„ tqdm ä¼šæ˜¾ç¤ºè¯»å– Key çš„è¿›åº¦
                for key, _ in tqdm.tqdm(curs, total=self.length, desc="Loading Keys", unit="it"):
                    self.keys.append(key)
        self.env.close()
        self.env = None 

    def __len__(self):
        return self.length

    def __getitem__(self, index):
        if self.env is None:
            self.env = lmdb.open(self.lmdb_path, readonly=True, lock=False)
        
        key = self.keys[index]
        with self.env.begin() as txn:
            value = txn.get(key)
        
        try:
            item = msgpack_numpy.unpackb(value, raw=False)
            if isinstance(item, (list, tuple)) and len(item) > 0:
                obs = item[0]
                if 'instruction' in obs:
                    instr = obs['instruction']
                    return np.max(instr).item()
        except Exception:
            pass
        return 0 

def check_vocab_limit_torch():
    if not os.path.exists(lmdb_path):
        print(f"âŒ é”™è¯¯ï¼šè·¯å¾„ä¸å­˜åœ¨ -> {lmdb_path}")
        return

    # 1. åˆå§‹åŒ– Dataset (è¿™é‡Œä¼šæ˜¾ç¤ºç¬¬ä¸€ä¸ªè¿›åº¦æ¡)
    dataset = VocabCheckDataset(lmdb_path)

    print(f"\nğŸš€ [é˜¶æ®µ2/2] å¼€å§‹å¤šè¿›ç¨‹æ‰«æ (Workers: {os.cpu_count()})...")
    
    # 2. åˆ›å»º DataLoader
    dataloader = DataLoader(
        dataset, 
        batch_size=2048, # è°ƒå¤§ Batch Size è®©è¿›åº¦æ¡è·‘å¾—æ›´é¡ºæ»‘
        shuffle=False, 
        num_workers=os.cpu_count(),
        collate_fn=lambda x: max(x) 
    )

    global_max = 0
    
    # 3. æ‰«æè¿‡ç¨‹ (è¿™é‡Œä¼šæ˜¾ç¤ºç¬¬äºŒä¸ªè¿›åº¦æ¡)
    # unit="batch" è®©ä½ çŸ¥é“å¤„ç†äº†å¤šå°‘ä¸ªæ‰¹æ¬¡
    for batch_max in tqdm.tqdm(dataloader, desc="Scanning Tokens", unit="batch"):
        if batch_max > global_max:
            global_max = batch_max
            
    print("\n" + "="*40)
    print(f"ğŸ”¢ æ‰«æç»“æœ - æœ€å¤§ Token ID: {global_max}")
    print(f"ğŸ›¡ï¸ æ¨¡å‹ Embedding å¤§å°: {vocab_size_limit}")
    
    if global_max < vocab_size_limit:
        print("\nğŸ‰ éªŒè¯æˆåŠŸï¼æ‰€æœ‰ Token éƒ½åœ¨å®‰å…¨èŒƒå›´å†…ã€‚")
    else:
        print(f"\nâŒ éªŒè¯å¤±è´¥ï¼æœ€å¤§ ID {global_max} è¶…è¿‡äº†é™åˆ¶ã€‚")
        print("ğŸ’¡ å¿…é¡»ä¿ç•™ InstructionEncoder é‡Œçš„æˆªæ–­ä»£ç ã€‚")
    print("="*40)

if __name__ == "__main__":
    torch.multiprocessing.set_start_method('spawn', force=True)
    check_vocab_limit_torch()
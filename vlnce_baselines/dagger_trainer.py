import gc
import os
import random
import warnings
from collections import defaultdict
import torch.distributed as dist
import lmdb
import msgpack_numpy
import numpy as np
import torch
import tqdm
from torch.nn.parallel import DistributedDataParallel as DDP
from habitat import logger
from habitat_baselines.common.baseline_registry import baseline_registry
from habitat_baselines.common.environments import get_env_class
from habitat_baselines.common.obs_transformers import (
    apply_obs_transforms_batch,
)
from habitat_baselines.common.tensorboard_utils import TensorboardWriter
from habitat_baselines.utils.common import batch_obs

from vlnce_baselines.common.aux_losses import AuxLosses
from vlnce_baselines.common.base_il_trainer import BaseVLNCETrainer
from vlnce_baselines.common.env_utils import construct_envs
from vlnce_baselines.common.utils import extract_instruction_tokens

from torch.cuda.amp import autocast, GradScaler # ğŸ”¥ æ–°å¢å¯¼å…¥
with warnings.catch_warnings():
    warnings.filterwarnings("ignore", category=FutureWarning)
    # import tensorflow as tf  # noqa: F401


class ObservationsDict(dict):
    def pin_memory(self):
        for k, v in self.items():
            self[k] = v.pin_memory()

        return self


def collate_fn(batch):
    """Each sample in batch: (
        obs,
        prev_actions,
        oracle_actions,
        inflec_weight,
    )
    """

    def _pad_helper(t, max_len, fill_val=0):
        pad_amount = max_len - t.size(0)
        if pad_amount == 0:
            return t

        pad = torch.full_like(t[0:1], fill_val).expand(
            pad_amount, *t.size()[1:]
        )
        return torch.cat([t, pad], dim=0)

    transposed = list(zip(*batch))

    observations_batch = list(transposed[0])
    prev_actions_batch = list(transposed[1])
    corrected_actions_batch = list(transposed[2])
    weights_batch = list(transposed[3])
    B = len(prev_actions_batch)

    new_observations_batch = defaultdict(list)
    for sensor in observations_batch[0]:
        for bid in range(B):
            new_observations_batch[sensor].append(
                observations_batch[bid][sensor]
            )

    observations_batch = new_observations_batch

    max_traj_len = max(ele.size(0) for ele in prev_actions_batch)
    for bid in range(B):
        for sensor in observations_batch:
            observations_batch[sensor][bid] = _pad_helper(
                observations_batch[sensor][bid], max_traj_len, fill_val=1.0
            )

        prev_actions_batch[bid] = _pad_helper(
            prev_actions_batch[bid], max_traj_len
        )
        corrected_actions_batch[bid] = _pad_helper(
            corrected_actions_batch[bid], max_traj_len
        )
        weights_batch[bid] = _pad_helper(weights_batch[bid], max_traj_len)

    for sensor in observations_batch:
        observations_batch[sensor] = torch.stack(
            observations_batch[sensor], dim=1
        )
        observations_batch[sensor] = observations_batch[sensor].view(
            -1, *observations_batch[sensor].size()[2:]
        )

    prev_actions_batch = torch.stack(prev_actions_batch, dim=1)
    corrected_actions_batch = torch.stack(corrected_actions_batch, dim=1)
    weights_batch = torch.stack(weights_batch, dim=1)
    not_done_masks = torch.ones_like(
        corrected_actions_batch, dtype=torch.uint8
    )
    not_done_masks[0] = 0

    observations_batch = ObservationsDict(observations_batch)

    return (
        observations_batch,
        prev_actions_batch.view(-1, 1),
        not_done_masks.view(-1, 1),
        corrected_actions_batch,
        weights_batch,
    )


def _block_shuffle(lst, block_size):
    blocks = [lst[i : i + block_size] for i in range(0, len(lst), block_size)]
    random.shuffle(blocks)

    return [ele for block in blocks for ele in block]


class IWTrajectoryDataset(torch.utils.data.IterableDataset):
    def __init__(
        self,
        lmdb_features_dir,
        use_iw,
        inflection_weight_coef=1.0,
        lmdb_map_size=1e9,
        batch_size=1,
    ):
        super().__init__()
        self.lmdb_features_dir = lmdb_features_dir
        self.lmdb_map_size = lmdb_map_size
        self.preload_size = batch_size * 100
        self._preload = []
        self.batch_size = batch_size

        if use_iw:
            self.inflec_weights = torch.tensor([1.0, inflection_weight_coef])
        else:
            self.inflec_weights = torch.tensor([1.0, 1.0])

        with lmdb.open(
            self.lmdb_features_dir,
            map_size=int(self.lmdb_map_size),
            readonly=True,
            lock=False,
        ) as lmdb_env:
            self.length = lmdb_env.stat()["entries"]

    def _load_next(self):
        if len(self._preload) == 0:
            if len(self.load_ordering) == 0:
                raise StopIteration

            new_preload = []
            lengths = []
            with lmdb.open(
                self.lmdb_features_dir,
                map_size=int(self.lmdb_map_size),
                readonly=True,
                lock=False,
            ) as lmdb_env, lmdb_env.begin(buffers=True) as txn:
                for _ in range(self.preload_size):
                    if len(self.load_ordering) == 0:
                        break

                    new_preload.append(
                        msgpack_numpy.unpackb(
                            txn.get(str(self.load_ordering.pop()).encode()),
                            raw=False,
                        )
                    )

                    lengths.append(len(new_preload[-1][0]))

            sort_priority = list(range(len(lengths)))
            random.shuffle(sort_priority)

            sorted_ordering = list(range(len(lengths)))
            sorted_ordering.sort(key=lambda k: (lengths[k], sort_priority[k]))

            for idx in _block_shuffle(sorted_ordering, self.batch_size):
                self._preload.append(new_preload[idx])

        return self._preload.pop()

    def __next__(self):
        obs, prev_actions, oracle_actions = self._load_next()

        for k, v in obs.items():
            obs[k] = torch.from_numpy(np.copy(v))

        prev_actions = torch.from_numpy(np.copy(prev_actions))
        oracle_actions = torch.from_numpy(np.copy(oracle_actions))

        inflections = torch.cat(
            [
                torch.tensor([1], dtype=torch.long),
                (oracle_actions[1:] != oracle_actions[:-1]).long(),
            ]
        )

        return (
            obs,
            prev_actions,
            oracle_actions,
            self.inflec_weights[inflections],
        )
    # ä¸‹é¢æ˜¯åŸæ¥çš„ä»£ç 
    # def __iter__(self):
    #     worker_info = torch.utils.data.get_worker_info()
    #     if worker_info is None:
    #         start = 0
    #         end = self.length
    #     else:
    #         per_worker = int(np.ceil(self.length / worker_info.num_workers))

    #         start = per_worker * worker_info.id
    #         end = min(start + per_worker, self.length)

    #     # Reverse so we can use .pop()
    #     self.load_ordering = list(
    #         reversed(
    #             _block_shuffle(list(range(start, end)), self.preload_size)
    #         )
    #     )

    #     return self
    # åŸæ¥çš„ä»£ç ç»“æŸ

    # ä¸‹é¢æ˜¯ä¿®æ”¹åçš„ä»£ç 
    def __iter__(self):
        # 1. è·å–åˆ†å¸ƒå¼ä¿¡æ¯ (DDP)
        if dist.is_initialized():
            rank = dist.get_rank()
            world_size = dist.get_world_size()
        else:
            rank = 0
            world_size = 1

        # 2. è·å– worker ä¿¡æ¯ (DataLoader num_workers)
        worker_info = torch.utils.data.get_worker_info()
        
        # 3. è®¡ç®—è¿™ä¸€å— GPU åº”è¯¥è´Ÿè´£çš„æ€»åŒºé—´ (GPU Sharding)
        # å°†æ•´ä¸ªæ•°æ®é›†å¹³å‡åˆ†æˆ world_size ä»½
        per_gpu_length = int(np.ceil(self.length / world_size))
        gpu_start = rank * per_gpu_length
        gpu_end = min(gpu_start + per_gpu_length, self.length)

        # 4. åœ¨è¿™å— GPU çš„åŒºé—´å†…ï¼Œå†åˆ†é…ç»™ä¸åŒçš„ CPU worker (Worker Sharding)
        if worker_info is None:
            # å•è¿›ç¨‹è¯»å–
            start = gpu_start
            end = gpu_end
        else:
            # å¤šè¿›ç¨‹è¯»å–ï¼šè®¡ç®—å½“å‰ GPU åŒºé—´å†…çš„åˆ‡ç‰‡
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåœ¨ gpu_start åˆ° gpu_end çš„èŒƒå›´å†…å†åˆ‡åˆ†
            valid_length = gpu_end - gpu_start
            per_worker = int(np.ceil(valid_length / worker_info.num_workers))
            
            worker_id = worker_info.id
            start = gpu_start + worker_id * per_worker
            end = min(start + per_worker, gpu_end)

        # 5. ç”ŸæˆåŠ è½½é¡ºåº
        # Reverse so we can use .pop()
        # æ³¨æ„ï¼šè¿™é‡Œ range çš„èŒƒå›´å·²ç»æ˜¯åˆ‡åˆ†å¥½çš„ [start, end)
        if start >= end:
            self.load_ordering = [] # è¿™ä¸ª worker/rank ä¸éœ€è¦å¹²æ´»
        else:
            self.load_ordering = list(
                reversed(
                    _block_shuffle(list(range(start, end)), self.preload_size)
                )
            )

        return self
    # ä¿®æ”¹ç»“æŸ

@baseline_registry.register_trainer(name="dagger")
class DaggerTrainer(BaseVLNCETrainer):
    def __init__(self, config=None):
        self.lmdb_features_dir = config.IL.DAGGER.lmdb_features_dir.format(
            split=config.TASK_CONFIG.DATASET.SPLIT
        )
        super().__init__(config)

        # ğŸ”¥ æ–°å¢ï¼šåˆå§‹åŒ–æ¢¯åº¦ç¼©æ”¾å™¨ï¼ˆç”¨äºæ··åˆç²¾åº¦ï¼‰
        self.scaler = GradScaler()
    def _update_agent(
        self,
        observations,
        prev_actions,
        not_done_masks,
        corrected_actions,
        weights,
        step_grad: bool = True,
        loss_accumulation_scalar: int = 1,
    ):
        T, N = corrected_actions.size()

        # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº† DDP
        net = self.policy.net.module if hasattr(self.policy.net, "module") else self.policy.net

        recurrent_hidden_states = torch.zeros(
            N,
            net.num_recurrent_layers,
            self.config.MODEL.STATE_ENCODER.hidden_size,
            device=self.device,
        )

        AuxLosses.clear()

        # ğŸ”¥ 1. å¼€å¯å‰å‘ä¼ æ’­çš„è‡ªåŠ¨æ··åˆç²¾åº¦
        with autocast():
            distribution = self.policy.build_distribution(
                observations, recurrent_hidden_states, prev_actions, not_done_masks
            )

            logits = distribution.logits
            logits = logits.view(T, N, -1)

            # äº¤å‰ç†µè®¡ç®— (åœ¨ autocast ä¸‹ä¼šè‡ªåŠ¨å¤„ç†ä¸ºç¨³å®šç²¾åº¦)
            action_loss = F.cross_entropy(
                logits.permute(0, 2, 1), corrected_actions, reduction="none"
            )
            action_loss = ((weights * action_loss).sum(0) / weights.sum(0)).mean()

            aux_mask = (weights > 0).view(-1)
            aux_loss = AuxLosses.reduce(aux_mask)

            loss = action_loss + aux_loss
            loss = loss / loss_accumulation_scalar

        # ğŸ”¥ 2. ä½¿ç”¨ scaler ç¼©æ”¾æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­
        # ä»£æ›¿åŸæ¥çš„ loss.backward()
        self.scaler.scale(loss).backward()

        if step_grad:
            # å¦‚æœä½ æœ‰æ¢¯åº¦è£å‰ªï¼Œåœ¨è¿™é‡Œæ·»åŠ ï¼š
            # self.scaler.unscale_(self.optimizer)
            # torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm)

            # ğŸ”¥ 3. ä½¿ç”¨ scaler.step æ›´æ–°å‚æ•°å¹¶æ›´æ–° scaler çŠ¶æ€
            # ä»£æ›¿åŸæ¥çš„ self.optimizer.step()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            self.optimizer.zero_grad()

        if isinstance(aux_loss, torch.Tensor):
            aux_loss = aux_loss.item()
            
        return loss.item(), action_loss.item(), aux_loss
    # ------------------ ğŸ”¥ æ–°å¢ä¿®å¤ä»£ç å¼€å§‹ ğŸ”¥ ------------------
    def load_checkpoint(self, checkpoint_path, *args, **kwargs):
        """
        è¦†ç›–çˆ¶ç±»çš„ load_checkpoint æ–¹æ³•ã€‚
        ä¸»è¦ç›®çš„æ˜¯åœ¨åŠ è½½æƒé‡å‰ï¼Œè‡ªåŠ¨å»é™¤ 'module.' å‰ç¼€ï¼Œ
        è§£å†³ä» DDP å¤šå¡è®­ç»ƒä¿å­˜çš„æ¨¡å‹åŠ è½½åˆ°å•å¡æˆ–å…¶ä»–ç¯å¢ƒæ—¶çš„ Key Mismatch é—®é¢˜ã€‚
        """
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint file not found: {checkpoint_path}")

        # 2. ä½¿ç”¨ torch.load åŠ è½½
        # map_location ç¡®ä¿åŠ è½½åˆ°å½“å‰è®¾å¤‡
        ckpt = torch.load(checkpoint_path, map_location=self.device)

        # 3. æ¸…æ´— state_dict ä¸­çš„ 'module.' å‰ç¼€
        if "state_dict" in ckpt:
            from collections import OrderedDict
            state_dict = ckpt["state_dict"]
            new_state_dict = OrderedDict()
            
            fixed_count = 0
            for k, v in state_dict.items():
                if k.startswith("module."):
                    name = k.replace("module.", "")
                    fixed_count += 1
                else:
                    name = k
                new_state_dict[name] = v
            
            # å°†æ¸…æ´—åçš„å­—å…¸æ”¾å› ckpt
            ckpt["state_dict"] = new_state_dict
            
            # æ‰“å°æ—¥å¿—ï¼ˆé˜²æ­¢æ‰€æœ‰è¿›ç¨‹éƒ½æ‰“å°ï¼Œåªè®©ä¸»è¿›ç¨‹æ‰“å°ï¼‰
            if not dist.is_initialized() or dist.get_rank() == 0:
                if fixed_count > 0:
                    logger.info(f"âœ… Fixed {fixed_count} keys by removing 'module.' prefix from {checkpoint_path}")
                else:
                    logger.info(f"Loaded checkpoint from {checkpoint_path} (No prefix fix needed).")

        return ckpt
    # ------------------ ğŸ”¥ æ–°å¢ä¿®å¤ä»£ç ç»“æŸ ğŸ”¥ ------------------

    def _make_dirs(self) -> None:
        self._make_ckpt_dir()
        os.makedirs(self.lmdb_features_dir, exist_ok=True)
        if self.config.EVAL.SAVE_RESULTS:
            self._make_results_dir()

    def _update_dataset(self, data_it):
        if torch.cuda.is_available():
            with torch.cuda.device(self.device):
                torch.cuda.empty_cache()

        envs = construct_envs(self.config, get_env_class(self.config.ENV_NAME))
        expert_uuid = self.config.IL.DAGGER.expert_policy_sensor_uuid
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ã€ä¿®å¤å¼€å§‹ã€‘å¤„ç† DDP æ¨¡å‹è§£åŒ… ğŸ”¥ğŸ”¥ğŸ”¥
        # è·å–åº•å±‚çš„æ¨¡å‹ (unwrapped)ï¼Œä»¥ä¾¿è®¿é—® num_recurrent_layers ç­‰å±æ€§
        # å¦‚æœ self.policy.net æ˜¯ DDP å¯¹è±¡ï¼Œå– .moduleï¼›å¦åˆ™ç›´æ¥ç”¨
        net_module = self.policy.net
        if hasattr(net_module, "module"):
            net_module = net_module.module
        # ğŸ”¥ğŸ”¥ğŸ”¥ã€ä¿®å¤ç»“æŸã€‘ğŸ”¥ğŸ”¥ğŸ”¥


        rnn_states = torch.zeros(
            envs.num_envs,
            net_module.num_recurrent_layers, # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨ net_module
            # self.policy.net.num_recurrent_layers,
            self.config.MODEL.STATE_ENCODER.hidden_size,
            device=self.device,
        )
        prev_actions = torch.zeros(
            envs.num_envs,
            1,
            device=self.device,
            dtype=torch.long,
        )
        not_done_masks = torch.zeros(
            envs.num_envs, 1, dtype=torch.uint8, device=self.device
        )

        observations = envs.reset()
        observations = extract_instruction_tokens(
            observations, self.config.TASK_CONFIG.TASK.INSTRUCTION_SENSOR_UUID
        )
        batch = batch_obs(observations, self.device)
        batch = apply_obs_transforms_batch(batch, self.obs_transforms)

        episodes = [[] for _ in range(envs.num_envs)]
        skips = [False for _ in range(envs.num_envs)]
        # Populate dones with False initially
        dones = [False for _ in range(envs.num_envs)]

        # https://arxiv.org/pdf/1011.0686.pdf
        # Theoretically, any beta function is fine so long as it converges to
        # zero as data_it -> inf. The paper suggests starting with beta = 1 and
        # exponential decay.
        p = self.config.IL.DAGGER.p
        # in Python 0.0 ** 0.0 == 1.0, but we want 0.0
        beta = 0.0 if p == 0.0 else p ** data_it

        ensure_unique_episodes = beta == 1.0

        def hook_builder(tgt_tensor):
            def hook(m, i, o):
                tgt_tensor.set_(o.cpu())

            return hook

        rgb_features = None
        rgb_hook = None
        if not self.config.MODEL.RGB_ENCODER.trainable:
            rgb_features = torch.zeros((1,), device="cpu")
            # rgb_hook = self.policy.net.rgb_encoder.cnn.register_forward_hook(
            #     hook_builder(rgb_features)
            # )
            # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨ net_module é˜²æ­¢ DDP ä¸‹æ‰¾ä¸åˆ° encoder
            rgb_hook = net_module.rgb_encoder.cnn.register_forward_hook(
                hook_builder(rgb_features)
            )

        depth_features = None
        depth_hook = None
        if not self.config.MODEL.DEPTH_ENCODER.trainable:
            depth_features = torch.zeros((1,), device="cpu")
            # depth_hook = self.policy.net.depth_encoder.visual_encoder.register_forward_hook(
            #     hook_builder(depth_features)
            # )
            # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨ net_module é˜²æ­¢ DDP ä¸‹æ‰¾ä¸åˆ° encoder
            depth_hook = net_module.depth_encoder.visual_encoder.register_forward_hook(
                hook_builder(depth_features)
            )

        collected_eps = 0
        ep_ids_collected = None
        if ensure_unique_episodes:
            ep_ids_collected = {
                ep.episode_id for ep in envs.current_episodes()
            }

        with tqdm.tqdm(
            total=self.config.IL.DAGGER.update_size, dynamic_ncols=True
        ) as pbar, lmdb.open(
            self.lmdb_features_dir,
            map_size=int(self.config.IL.DAGGER.lmdb_map_size),
        ) as lmdb_env, torch.no_grad():
            
            # # âœ… 1. è®¡ç®—è¿˜éœ€è¦è·‘å¤šå°‘æ¡
            # start_id = lmdb_env.stat()["entries"]
            # # åŸå§‹ä»£ç ï¼šcollected_eps = 0
            # # ä¿®æ”¹åï¼šæˆ‘ä»¬åªæ”¶é›†â€œå‰©ä¸‹â€çš„éƒ¨åˆ†
            # target_size = self.config.IL.DAGGER.update_size # ç›®æ ‡æ€»æ•° (157232)
            # needed_eps = target_size - start_id # è¿˜éœ€è¦è·‘å¤šå°‘ (ä¾‹å¦‚å‰©ä¸‹ 10000)
            # # å¦‚æœå·²ç»æ”¶é›†å¤Ÿäº†ï¼Œç›´æ¥è¿”å›
            # if needed_eps <= 0:
            #     logger.info("Data collection complete. Skipping.")
            #     envs.close()
            #     return

            # 1. è·å–å½“å‰æ•°æ®åº“é‡Œå·²ç»æœ‰çš„æ¡æ•°
            start_id = lmdb_env.stat()["entries"]

            # 2. ã€æ ¸å¿ƒä¿®æ”¹ã€‘è®¡ç®—è¿™ä¸€è½®ç»“æŸæ—¶ï¼Œæ•°æ®åº“åº”è¯¥æœ‰çš„æ€»æ¡æ•°
            # data_it æ˜¯ä» 0 å¼€å§‹çš„ (Iter 0, Iter 1...)
            # Iter 0 ç»“æŸåº”æœ‰ 1 * update_size
            # Iter 1 ç»“æŸåº”æœ‰ 2 * update_size
            per_iter_size = self.config.IL.DAGGER.update_size
            target_cumulative_size = per_iter_size * (data_it + 1)

            # 3. è®¡ç®—ç¼ºå£ï¼šè¿˜éœ€è¦è¡¥å¤šå°‘æ¡ï¼Ÿ
            needed_eps = target_cumulative_size - start_id
            
            # æ‰“å°æ—¥å¿—æ–¹ä¾¿è°ƒè¯•
            info_msg = f"DAgger Iter: {data_it} | Existing: {start_id} | Target Total: {target_cumulative_size} | Needed: {needed_eps}"
            if dist.is_initialized():
                if dist.get_rank() == 0: logger.info(info_msg)
            else:
                logger.info(info_msg)

            # 4. å¦‚æœç¼ºå£ <= 0ï¼Œè¯´æ˜è¿™ä¸€è½®çš„æ•°æ®ä»¥å‰å·²ç»è·‘è¿‡äº†ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰ï¼Œç›´æ¥è·³è¿‡
            if needed_eps <= 0:
                logger.info("Data already sufficient for this iteration. Skipping collection.")
                envs.close()
                # è®°å¾—ç§»é™¤ hook é˜²æ­¢å†…å­˜æ³„æ¼
                if rgb_hook is not None: rgb_hook.remove()
                if depth_hook is not None: depth_hook.remove()
                return

            # 5. é‡ç½®è¿›åº¦æ¡ï¼Œåªæ˜¾ç¤ºæœ¬æ¬¡éœ€è¦é‡‡é›†çš„æ•°é‡
            pbar.reset(total=needed_eps)
            # pbar.update(start_id) # æ›´æ–°è¿›åº¦æ¡åˆ°å½“å‰ä½ç½®
            txn = lmdb_env.begin(write=True)
            # âœ… è¿™é‡Œçš„ collected_eps å¿…é¡»ä» 0 å¼€å§‹ï¼
            # å› ä¸ºä¸‹é¢çš„ txn.put ç”¨çš„æ˜¯ start_id + collected_eps
            collected_eps = 0 

            # âœ… 2. ä¿®æ”¹å¾ªç¯æ¡ä»¶ï¼Œåªè·‘å‰©ä¸‹çš„é‡
            while collected_eps < needed_eps:
            # ä¿®æ”¹ç»“æŸ

            # ä¸‹é¢ä¸‰è¡Œæ˜¯å®˜æ–¹è€ä»£ç 
            # start_id = lmdb_env.stat()["entries"]
            # txn = lmdb_env.begin(write=True)

            # while collected_eps < self.config.IL.DAGGER.update_size:
                current_episodes = None
                envs_to_pause = None
                if ensure_unique_episodes:
                    envs_to_pause = []
                    current_episodes = envs.current_episodes()

                for i in range(envs.num_envs):
                    if dones[i] and not skips[i]:
                        ep = episodes[i]
                        traj_obs = batch_obs(
                            [step[0] for step in ep],
                            device=torch.device("cpu"),
                        )
                        del traj_obs[expert_uuid]
                        for k, v in traj_obs.items():
                            traj_obs[k] = v.numpy()
                            if self.config.IL.DAGGER.lmdb_fp16:
                                traj_obs[k] = traj_obs[k].astype(np.float16)

                        transposed_ep = [
                            traj_obs,
                            np.array([step[1] for step in ep], dtype=np.int64),
                            np.array([step[2] for step in ep], dtype=np.int64),
                        ]
                        txn.put(
                            str(start_id + collected_eps).encode(),
                            msgpack_numpy.packb(
                                transposed_ep, use_bin_type=True
                            ),
                        )

                        pbar.update()
                        collected_eps += 1

                        # ------------------ æ–°å¢ä»£ç å¼€å§‹ ------------------
                        # æ¯é‡‡é›† 50 ä¸ª episode å¼ºåˆ¶æ¸…ç†ä¸€æ¬¡å†…å­˜
                        # è¿™èƒ½æœ‰æ•ˆé˜²æ­¢å†…å­˜ç¢ç‰‡åŒ–å¯¼è‡´çš„ OOM
                        if collected_eps % 5000 == 0:
                            gc.collect()
                        # ------------------ æ–°å¢ä»£ç ç»“æŸ ------------------

                        if (
                            collected_eps
                            % self.config.IL.DAGGER.lmdb_commit_frequency
                        ) == 0:
                            txn.commit()
                            txn = lmdb_env.begin(write=True)

                        if ensure_unique_episodes:
                            if (
                                current_episodes[i].episode_id
                                in ep_ids_collected
                            ):
                                envs_to_pause.append(i)
                            else:
                                ep_ids_collected.add(
                                    current_episodes[i].episode_id
                                )

                    if dones[i]:
                        episodes[i] = []

                if ensure_unique_episodes:
                    (
                        envs,
                        rnn_states,
                        not_done_masks,
                        prev_actions,
                        batch,
                        _,
                    ) = self._pause_envs(
                        envs_to_pause,
                        envs,
                        rnn_states,
                        not_done_masks,
                        prev_actions,
                        batch,
                    )
                    if envs.num_envs == 0:
                        break

                actions, rnn_states = self.policy.act(
                    batch,
                    rnn_states,
                    prev_actions,
                    not_done_masks,
                    deterministic=False,
                )
                actions = torch.where(
                    torch.rand_like(actions, dtype=torch.float) < beta,
                    batch[expert_uuid].long(),
                    actions,
                )

                for i in range(envs.num_envs):
                    if rgb_features is not None:
                        observations[i]["rgb_features"] = rgb_features[i]
                        del observations[i]["rgb"]

                    if depth_features is not None:
                        observations[i]["depth_features"] = depth_features[i]
                        del observations[i]["depth"]

                    episodes[i].append(
                        (
                            observations[i],
                            prev_actions[i].item(),
                            batch[expert_uuid][i].item(),
                        )
                    )

                skips = batch[expert_uuid].long() == -1
                actions = torch.where(
                    skips, torch.zeros_like(actions), actions
                )
                skips = skips.squeeze(-1).to(device="cpu", non_blocking=True)
                prev_actions.copy_(actions)

                outputs = envs.step([a[0].item() for a in actions])
                observations, _, dones, _ = [list(x) for x in zip(*outputs)]
                observations = extract_instruction_tokens(
                    observations,
                    self.config.TASK_CONFIG.TASK.INSTRUCTION_SENSOR_UUID,
                )
                batch = batch_obs(observations, self.device)
                batch = apply_obs_transforms_batch(batch, self.obs_transforms)

                not_done_masks = torch.tensor(
                    [[0] if done else [1] for done in dones],
                    dtype=torch.uint8,
                    device=self.device,
                )

            txn.commit()

        envs.close()
        envs = None

        if rgb_hook is not None:
            rgb_hook.remove()
        if depth_hook is not None:
            depth_hook.remove()

    def train(self) -> None:
        """Main method for training DAgger."""
        world_size = dist.get_world_size() if dist.is_initialized() else 1
        if self.config.IL.DAGGER.preload_lmdb_features:
            try:
                lmdb.open(self.lmdb_features_dir, readonly=True, lock=False)
            except lmdb.Error as err:
                logger.error(
                    "Cannot open database for teacher forcing preload."
                )
                raise err
        else:
            # ä¸‹é¢äº”è¡Œæ˜¯å®˜æ–¹è€ä»£ç 
            # with lmdb.open(
            #     self.lmdb_features_dir,
            #     map_size=int(self.config.IL.DAGGER.lmdb_map_size),
            # ) as lmdb_env, lmdb_env.begin(write=True) as txn:
            #     txn.drop(lmdb_env.open_db())

            # ä¸‹é¢æ˜¯æ–°çš„ä»£ç 
            # âœ… æ–°å¢é€»è¾‘ï¼šåªæœ‰å½“æ•°æ®åº“ä¸ºç©ºæˆ–ä¸å­˜åœ¨æ—¶ï¼Œæ‰æ‰§è¡Œæ¸…ç©ºæ“ä½œ
            # âš ï¸ã€ä¿®æ”¹å¼€å§‹ã€‘æ–­ç‚¹ç»­ä¼ é€»è¾‘
            # å…ˆä»¥åªè¯»æ¨¡å¼æ‰“å¼€ï¼Œçœ‹çœ‹é‡Œé¢æœ‰å¤šå°‘æ•°æ®
            current_entries = 0
            # 1. æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¹¶ä¸”æ£€æŸ¥é‡Œé¢æ˜¯å¦æœ‰æ–‡ä»¶
            # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œæˆ–è€…æ–‡ä»¶å¤¹å­˜åœ¨ä½†ä¸ºç©ºåˆ—è¡¨ï¼ˆ[]ï¼‰ï¼Œéƒ½ç®—ä½œ 0
            if os.path.exists(self.lmdb_features_dir) and len(os.listdir(self.lmdb_features_dir)) > 0:
                try:
                    with lmdb.open(self.lmdb_features_dir, readonly=True, lock=False) as lmdb_env:
                        current_entries = lmdb_env.stat()["entries"]
                except (lmdb.Error, Exception):
                    # å¦‚æœæ–‡ä»¶æŸåæˆ–æ‰“ä¸å¼€ï¼Œä¹Ÿé‡ç½®ä¸º 0
                    logger.info("Existing LMDB is corrupted or empty. Starting from scratch.")
                    current_entries = 0
            # åªæœ‰å½“æ•°æ®æ˜¯ 0 æˆ–è€…ä¸å­˜åœ¨æ—¶ï¼Œæ‰æ‰§è¡Œ drop (æ¸…ç©º)
            # å¦åˆ™æˆ‘ä»¬è®¤ä¸ºæ˜¯æƒ³æ¥ç€è·‘
            if current_entries == 0:
                with lmdb.open(
                    self.lmdb_features_dir,
                    map_size=int(self.config.IL.DAGGER.lmdb_map_size),
                ) as lmdb_env, lmdb_env.begin(write=True) as txn:
                    txn.drop(lmdb_env.open_db())
                logger.info("Created new LMDB database.")
            else:
                logger.info(f"Found {current_entries} entries, resuming...")
            # âš ï¸ã€ä¿®æ”¹ç»“æŸã€‘æ–­ç‚¹ç»­ä¼ é€»è¾‘

        EPS = self.config.IL.DAGGER.expert_policy_sensor
        if EPS not in self.config.TASK_CONFIG.TASK.SENSORS:
            self.config.TASK_CONFIG.TASK.SENSORS.append(EPS)

        self.config.defrost()

        # if doing teacher forcing, don't switch the scene until it is complete
        if self.config.IL.DAGGER.p == 1.0:
            self.config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS = (
                -1
            )
        self.config.freeze()

        observation_space, action_space = self._get_spaces(self.config)

        self._initialize_policy(
            self.config,
            self.config.IL.load_from_ckpt,
            observation_space=observation_space,
            action_space=action_space,
        )
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ã€æ–°å¢ã€‘DDP æ¨¡å‹åŒ…è£…ï¼šå®ç°æ¢¯åº¦çš„è‡ªåŠ¨ç»“åˆ ğŸ”¥ğŸ”¥ğŸ”¥
        if dist.is_initialized():
            # è·å–å½“å‰è®¾å¤‡
            device_id = self.device
            # åŒ…è£…æ¨¡å‹
            # find_unused_parameters=True æ˜¯ä¸ºäº†é˜²æ­¢å› ä¸º CLIP å†»ç»“å‚æ•°å¯¼è‡´çš„æŠ¥é”™
            self.policy.net = DDP(
                self.policy.net, 
                device_ids=[device_id], 
                output_device=device_id, 
                find_unused_parameters=False
            )
            logger.info(f"Process {dist.get_rank()}: Wrapped model with DDP")
        # ğŸ”¥ğŸ”¥ğŸ”¥ã€æ–°å¢ã€‘DDP æ¨¡å‹åŒ…è£…ç»“æŸ ğŸ”¥ğŸ”¥ğŸ”¥
        
        with TensorboardWriter(
            self.config.TENSORBOARD_DIR,
            flush_secs=self.flush_secs,
            purge_step=0,
        ) as writer:
            for dagger_it in range(self.config.IL.DAGGER.iterations):

                # ------------------ ğŸ”¥ æ–°å¢ä¿®å¤ä»£ç å¼€å§‹ (é€»è¾‘æ§åˆ¶) ğŸ”¥ ------------------
                # è®¡ç®—å½“å‰è¿™ä¸€è½® dagger_it çš„èµ·å§‹ epoch
                # å‡è®¾æ¯è½® DAgger è®­ç»ƒ 4 ä¸ª epoch (self.config.IL.epochs = 4)
                # è¿™é‡Œçš„ self.start_epoch æ˜¯å…¨å±€ç´¯ç§¯çš„ epoch (ä¾‹å¦‚åŠ è½½äº† epoch 6)
                
                epochs_per_iter = self.config.IL.epochs
                
                # 1. å¦‚æœè¿™ä¸€è½®å®Œå…¨æ˜¯ä»¥å‰è·‘è¿‡çš„ (ä¾‹å¦‚å½“å‰ dagger_it=0, ä½†æˆ‘ä»¬ä» epoch 6 æ¢å¤)
                # 6 // 4 = 1ï¼Œè¯´æ˜ Iter 0 å·²ç»è·‘å®Œäº†
                if self.config.IL.load_from_ckpt and dagger_it < (self.start_epoch // epochs_per_iter):
                    if dist.get_rank() == 0:
                        logger.info(f"Skipping DAgger Iter {dagger_it} (Already trained in previous run).")
                    # ç›´æ¥è·³è¿‡è¿™ä¸€è½®ï¼Œä¸é‡‡é›†æ•°æ®ï¼Œä¸åŠ è½½ Dataset
                    continue

                # 2. å¦‚æœè¿™ä¸€è½®æ˜¯â€œæ–­ç‚¹ç»­ä¼ â€çš„é‚£ä¸€è½® (ä¾‹å¦‚å½“å‰ dagger_it=1, ä» epoch 6 æ¢å¤)
                # æˆ‘ä»¬åº”è¯¥ä»ç¬¬ 2 ä¸ª epoch å¼€å§‹è·‘ (6 % 4 = 2)
                elif self.config.IL.load_from_ckpt and dagger_it == (self.start_epoch // epochs_per_iter):
                    current_start_epoch = self.start_epoch % epochs_per_iter
                    if not dist.is_initialized() or dist.get_rank() == 0:
                        logger.info(f"Resuming DAgger Iter {dagger_it} from Epoch {current_start_epoch}.")
                
                # 3. å¦‚æœæ˜¯å…¨æ–°çš„è½®æ¬¡ (ä¾‹å¦‚ dagger_it=2)
                else:
                    current_start_epoch = 0
                # ------------------ ğŸ”¥ æ–°å¢ä¿®å¤ä»£ç ç»“æŸ ğŸ”¥ ------------------    

                # âŒ åŸä»£ç æ˜¯: step_id = 0
                step_id = self.step_id # è¿™é‡ŒåŸæ¥æ˜¯0
                # åªæœ‰å½“è¿™æ˜¯å…¨æ–°çš„è®­ç»ƒï¼ˆé Resumeï¼‰ï¼Œä¸”æ˜¯ç¬¬ä¸€è½®æ—¶ï¼Œæ‰é‡ç½®ä¸º 0
                if not self.config.IL.is_requeue and dagger_it == 0 and not self.config.IL.load_from_ckpt:
                    step_id = 0
                # æ–°å¢ä»£ç ç»“æŸ
                if not self.config.IL.DAGGER.preload_lmdb_features:
                    self._update_dataset(
                        dagger_it + (1 if self.config.IL.load_from_ckpt else 0)
                    )

                if torch.cuda.is_available():
                    with torch.cuda.device(self.device):
                        torch.cuda.empty_cache()
                gc.collect()

                dataset = IWTrajectoryDataset(
                    self.lmdb_features_dir,
                    self.config.IL.use_iw,
                    inflection_weight_coef=self.config.IL.inflection_weight_coef,
                    lmdb_map_size=self.config.IL.DAGGER.lmdb_map_size,
                    batch_size=self.config.IL.batch_size,
                )
                diter = torch.utils.data.DataLoader(
                    dataset,
                    batch_size=self.config.IL.batch_size,
                    shuffle=False,
                    collate_fn=collate_fn,
                    pin_memory=False,
                    drop_last=True,  # drop last batch if smaller
                    num_workers=3, # åŸå…ˆæ˜¯3
                )

                AuxLosses.activate()
                for epoch in tqdm.trange(
                    current_start_epoch, self.config.IL.epochs, dynamic_ncols=True # æ–°å¢self.start_epoch
                ):
                    for batch in tqdm.tqdm(
                        diter,
                        total=(dataset.length // dataset.batch_size) // world_size,
                        leave=False,
                        dynamic_ncols=True,
                    ):
                        (
                            observations_batch,
                            prev_actions_batch,
                            not_done_masks,
                            corrected_actions_batch,
                            weights_batch,
                        ) = batch

                        observations_batch = {
                            k: v.to(
                                device=self.device,
                                dtype=torch.float32,
                                non_blocking=True,
                            )
                            for k, v in observations_batch.items()
                        }

                        loss, action_loss, aux_loss = self._update_agent(
                            observations_batch,
                            prev_actions_batch.to(
                                device=self.device, non_blocking=True
                            ),
                            not_done_masks.to(
                                device=self.device, non_blocking=True
                            ),
                            corrected_actions_batch.to(
                                device=self.device, non_blocking=True
                            ),
                            weights_batch.to(
                                device=self.device, non_blocking=True
                            ),
                        )
                        # ğŸ”¥ğŸ”¥ğŸ”¥ã€ä¿®æ”¹ã€‘åªè®© Rank 0 å†™æ—¥å¿— ğŸ”¥ğŸ”¥ğŸ”¥
                        if dist.is_initialized():
                            rank = dist.get_rank()
                        else:
                            rank = 0

                        # åªæœ‰ rank 0 è´Ÿè´£æ‰“å°å’Œå†™ TensorBoard
                        if rank == 0:
                            logger.info(f"train_loss: {loss}")
                            logger.info(f"train_action_loss: {action_loss}")
                            logger.info(f"train_aux_loss: {aux_loss}")
                            logger.info(f"Batches processed: {step_id}.")
                            logger.info(
                                f"On DAgger iter {dagger_it}, Epoch {epoch}."
                            )
                            writer.add_scalar(
                                f"train_loss_iter_{dagger_it}", loss, step_id
                            )
                            writer.add_scalar(
                                f"train_action_loss_iter_{dagger_it}",
                                action_loss,
                                step_id,
                            )
                            writer.add_scalar(
                                f"train_aux_loss_iter_{dagger_it}",
                                aux_loss,
                                step_id,
                            )

                        step_id += 1 
                        # ---------------------------------------------
                        # ä¸‹é¢æ˜¯è€ä»£ç 
                        # logger.info(f"train_loss: {loss}")
                        # logger.info(f"train_action_loss: {action_loss}")
                        # logger.info(f"train_aux_loss: {aux_loss}")
                        # logger.info(f"Batches processed: {step_id}.")
                        # logger.info(
                        #     f"On DAgger iter {dagger_it}, Epoch {epoch}."
                        # )
                        # writer.add_scalar(
                        #     f"train_loss_iter_{dagger_it}", loss, step_id
                        # )
                        # writer.add_scalar(
                        #     f"train_action_loss_iter_{dagger_it}",
                        #     action_loss,
                        #     step_id,
                        # )
                        # writer.add_scalar(
                        #     f"train_aux_loss_iter_{dagger_it}",
                        #     aux_loss,
                        #     step_id,
                        # )
                        # step_id += 1  # noqa: SIM113
                        # åŸæ¥çš„ä»£ç ç»“æŸ
                        # ------------------ æ–°å¢ä»£ç  ------------------
                        if step_id % 2000 == 0:  # æ¯è®­ç»ƒ2000ä¸ªbatchæ¸…ç†ä¸€æ¬¡
                            gc.collect()
                        # ---------------------------------------------

                    # ğŸ”¥ğŸ”¥ğŸ”¥ã€ä¿®æ”¹ã€‘åªè®© Rank 0 ä¿å­˜æ¨¡å‹ ğŸ”¥ğŸ”¥ğŸ”¥
                    if rank == 0:
                        self.save_checkpoint(
                            f"ckpt.{dagger_it * self.config.IL.epochs + epoch}.pth",
                            epoch=epoch,     # ä¼ å…¥å½“å‰ epoch
                            step_id=step_id  # ä¼ å…¥å½“å‰ step_id
                        )
                    # ä¸‹é¢æ˜¯è€ä»£ç 
                    # self.save_checkpoint(
                    #     f"ckpt.{dagger_it * self.config.IL.epochs + epoch}.pth"
                    # )
                AuxLosses.deactivate()

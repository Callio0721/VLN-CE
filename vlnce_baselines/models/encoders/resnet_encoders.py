from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from gym import Space, spaces
from habitat.core.simulator import Observations
from habitat_baselines.rl.ddppo.policy import resnet
from habitat_baselines.rl.ddppo.policy.resnet_policy import ResNetEncoder
from torch import Tensor

from vlnce_baselines.common.utils import single_frame_box_shape

# --- åœ¨ resnet_encoders.py å¤´éƒ¨æ·»åŠ  import ---
from transformers import CLIPVisionModel
import torchvision.transforms as T


class VlnResnetDepthEncoder(nn.Module):
    def __init__(
        self,
        observation_space: Space,
        output_size: int = 128,
        checkpoint: str = "NONE",
        backbone: str = "resnet50",
        resnet_baseplanes: int = 32,
        normalize_visual_inputs: bool = False,
        trainable: bool = False,
        spatial_output: bool = False,
    ) -> None:
        super().__init__()

        self.visual_encoder = ResNetEncoder(
            spaces.Dict(
                {
                    "depth": single_frame_box_shape(
                        observation_space.spaces["depth"]
                    )
                }
            ),
            baseplanes=resnet_baseplanes,
            ngroups=resnet_baseplanes // 2,
            make_backbone=getattr(resnet, backbone),
            normalize_visual_inputs=normalize_visual_inputs,
        )

        for param in self.visual_encoder.parameters():
            param.requires_grad_(trainable)

        if checkpoint != "NONE":
            ddppo_weights = torch.load(checkpoint)

            weights_dict = {}
            for k, v in ddppo_weights["state_dict"].items():
                split_layer_name = k.split(".")[2:]
                if split_layer_name[0] != "visual_encoder":
                    continue

                layer_name = ".".join(split_layer_name[1:])
                weights_dict[layer_name] = v

            del ddppo_weights
            self.visual_encoder.load_state_dict(weights_dict, strict=True)

        self.spatial_output = spatial_output

        if not self.spatial_output:
            self.output_shape = (output_size,)
            self.visual_fc = nn.Sequential(
                nn.Flatten(),
                nn.Linear(
                    np.prod(self.visual_encoder.output_shape), output_size
                ),
                nn.ReLU(True),
            )
        else:
            self.spatial_embeddings = nn.Embedding(
                self.visual_encoder.output_shape[1]
                * self.visual_encoder.output_shape[2],
                64,
            )

            self.output_shape = list(self.visual_encoder.output_shape)
            self.output_shape[0] += self.spatial_embeddings.embedding_dim
            self.output_shape = tuple(self.output_shape)

    def forward(self, observations: Observations) -> Tensor:
        """
        Args:
            observations: [BATCH, HEIGHT, WIDTH, CHANNEL]
        Returns:
            [BATCH, OUTPUT_SIZE]
        """
        if "depth_features" in observations:
            x = observations["depth_features"]
        else:
            x = self.visual_encoder(observations)

        if self.spatial_output:
            b, c, h, w = x.size()

            spatial_features = (
                self.spatial_embeddings(
                    torch.arange(
                        0,
                        self.spatial_embeddings.num_embeddings,
                        device=x.device,
                        dtype=torch.long,
                    )
                )
                .view(1, -1, h, w)
                .expand(b, self.spatial_embeddings.embedding_dim, h, w)
            )

            return torch.cat([x, spatial_features], dim=1)
        else:
            return self.visual_fc(x)


class TorchVisionResNet(nn.Module):
    """TorchVision ResNet pre-trained on ImageNet. The standard average
    pooling can be replaced with spatial average pooling. The final fc layer
    is replaced with a new fc layer of a specified output size.
    """

    def __init__(
        self,
        output_size: int,
        resnet_version: str = "resnet50",
        normalize_visual_inputs: bool = False,
        trainable: bool = False,
        spatial_output: bool = False,
        single_spatial_filter: bool = True,
    ) -> None:
        super().__init__()
        self.normalize_visual_inputs = normalize_visual_inputs
        self.spatial_output = spatial_output
        resnet = getattr(models, resnet_version)(pretrained=True)
        modules = list(resnet.children())
        self.resnet_layer_size = modules[-1].in_features
        self.cnn = nn.Sequential(*modules[:-1])

        for param in self.cnn.parameters():
            param.requires_grad_(trainable)
        self.cnn.train(trainable)

        if not self.spatial_output:
            self.output_shape = (output_size,)
            self.fc = nn.Sequential(
                nn.Flatten(),
                nn.Linear(self.resnet_layer_size, output_size),
                nn.ReLU(),
            )
        else:

            class SpatialAvgPool(nn.Module):
                def forward(self, x):
                    x = F.adaptive_avg_pool2d(x, (4, 4))

                    return x

            if single_spatial_filter:
                self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])
            self.cnn.avgpool = SpatialAvgPool()
            self.spatial_embeddings = nn.Embedding(4 * 4, 64)
            self.output_shape = (
                self.resnet_layer_size + self.spatial_embeddings.embedding_dim,
                4,
                4,
            )

    def forward(self, observations: Observations) -> Tensor:
        def normalize(imgs: Tensor) -> Tensor:
            """Normalizes a batch of images by:
                1) scaling pixel values to be in the range 0-1
                2) subtracting the ImageNet mean
                3) dividing by the ImageNet variance
                TODO: could be nice to calculate mean and variance for Habitat MP3D scenes.
                    Method: compute for training split with oracle path follower.
            Args:
                imgs: must have pixel values ranging from 0-255. Assumes a size of [Bx3xHxW]
            https://github.com/pratogab/batch-transforms/blob/master/batch_transforms.py
            """
            imgs = imgs.contiguous() / 255.0
            if self.normalize_visual_inputs:
                mean_norm = torch.tensor([0.485, 0.456, 0.406]).to(
                    device=imgs.device
                )[None, :, None, None]
                std_norm = torch.tensor([0.229, 0.224, 0.225]).to(
                    device=imgs.device
                )[None, :, None, None]
                return imgs.sub(mean_norm).div(std_norm)
            else:
                return imgs

        if "rgb_features" in observations:
            resnet_output = observations["rgb_features"]
        else:
            # permute tensor to dimension [BATCH x CHANNEL x HEIGHT x WIDTH]
            rgb_observations = observations["rgb"].permute(0, 3, 1, 2)
            resnet_output = self.cnn(normalize(rgb_observations))

        if self.spatial_output:
            b, c, h, w = resnet_output.size()

            spatial_features = (
                self.spatial_embeddings(
                    torch.arange(
                        0,
                        self.spatial_embeddings.num_embeddings,
                        device=resnet_output.device,
                        dtype=torch.long,
                    )
                )
                .view(1, -1, h, w)
                .expand(b, self.spatial_embeddings.embedding_dim, h, w)
            )

            return torch.cat([resnet_output, spatial_features], dim=1)
        else:
            return self.fc(resnet_output)  # returns [BATCH x OUTPUT_DIM]


class TorchVisionResNet50(TorchVisionResNet):
    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, resnet_version="resnet50", **kwargs)


class TorchVisionResNet18(TorchVisionResNet):
    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, resnet_version="resnet18", **kwargs)


# --- åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ è¿™ä¸ªç±» ---
class ClipVisualEncoder(nn.Module):
    def __init__(
        self, 
        output_size,   # <--- æ”¹åŠ¨ç‚¹ï¼šå¿…é¡»æ”¾åœ¨ç¬¬ä¸€ä¸ªï¼ŒåŒ¹é… CMAPolicy çš„è°ƒç”¨
        observation_space=None, # æ”¹åŠ¨ç‚¹ï¼šå˜æˆå¯é€‰å‚æ•°ï¼Œæˆ–è€…ç›´æ¥åˆ æ‰ï¼Œç”¨ kwargs æ¥æ”¶
        model_name="/home/ShiKaituo/ZhangBodong/VLN-CE/clip-vit-base-patch32",
        trainable=False, 
        **kwargs       # <--- è¿™é‡Œä¼šå¸®ä½ â€œåæ‰â€ normalize_visual_inputs å’Œ spatial_output ç­‰å¤šä½™å‚æ•°ï¼Œé˜²æ­¢æŠ¥é”™
    ):
        super().__init__()
        print(f"Loading CLIP: {model_name}, Output Size: {output_size}")
        
        self.backbone = CLIPVisionModel.from_pretrained(model_name)
        
        # -----------------------------------------------------------
        # å…³é”®é€»è¾‘ï¼šå‚æ•°å†»ç»“ç­–ç•¥
        # -----------------------------------------------------------
        # 1. é¦–å…ˆï¼Œä¸ºäº†èŠ‚çœæ˜¾å­˜ï¼Œæˆ‘ä»¬æ€»æ˜¯å†»ç»“ CLIP çš„ä¸»å¹² (Backbone)
        # ä¸ç®¡é…ç½®æ–‡ä»¶æ€ä¹ˆå†™ï¼ŒCLIP è¿™ç§å¤§æ¨¡å‹åœ¨ VLN é‡Œä¸€èˆ¬éƒ½è·‘ä¸åŠ¨å¾®è°ƒ
        for param in self.backbone.parameters():
            param.requires_grad = False
            
        # 2. å®šä¹‰æŠ•å½±å±‚ï¼šæŠŠ CLIP çš„ 768 å˜æˆ Config è¦æ±‚çš„ 256
        self.projection = nn.Linear(768, output_size)
        
        # 3. å¤„ç† trainable é…ç½®
        # å¦‚æœ config è¯´ trainable=Trueï¼Œé‚£æ˜¯ä¸ºäº†è®­ç»ƒ projection å±‚ã€‚
        # å¦‚æœ config è¯´ trainable=Falseï¼Œé‚£ projection å±‚ä¹Ÿå°±æ²¡æ³•è®­ç»ƒäº† (è¿™æ˜¯ä¸å¯¹çš„)
        # æ‰€ä»¥ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåšä¸€ä¸ªå¼ºåˆ¶è¦†ç›–ï¼Œæˆ–è€…ä¾èµ– config å¿…é¡»è®¾ä¸º Trueã€‚
        
        # å»ºè®®ï¼šProjection å±‚æ°¸è¿œéœ€è¦è®­ç»ƒï¼Œå¦åˆ™å®ƒå°±æ˜¯éšæœºå™ªå£°
        # å³ä½¿ trainable=Falseï¼Œæˆ‘ä»¬ä¾ç„¶è®© projection å¯å¯¼ï¼Œ
        # ä½†è¿™å–å†³äº Optimizer æ˜¯å¦ä¼šæ‰«æåˆ°å®ƒã€‚
        # ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬åœ¨ yaml é‡Œå¿…é¡»æŠŠ trainable è®¾ä¸º Trueã€‚

        self.output_size = output_size
        self._output_shape = (output_size, 49) 

        # CLIP çš„æ ‡å‡†åŒ–
        # æ³¨å†Œ CLIP çš„å‡å€¼å’Œæ–¹å·®ï¼Œå½¢çŠ¶è®¾ä¸º [1, 3, 1, 1] ä»¥ä¾¿åˆ©ç”¨å¹¿æ’­æœºåˆ¶ç›´æ¥å¤„ç† Batch
        self.register_buffer(
            "clip_mean", 
            torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1, 3, 1, 1)
        )
        self.register_buffer(
            "clip_std", 
            torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1, 3, 1, 1)
        )

    @property
    def is_blind(self):
        return False
        
    @property
    def output_shape(self):
        return self._output_shape


    def forward(self, observations):
            """
            observations["rgb"]: [Batch, H, W, 3] (uint8 0-255)
            """
            # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢é€»è¾‘ï¼šä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¡ç®—çš„ç‰¹å¾ ğŸ”¥ğŸ”¥ğŸ”¥
            # å¦‚æœ DAgger å·²ç»æŠŠ Backbone çš„ç‰¹å¾è·‘å‡ºæ¥å¹¶å­˜å¥½äº†ï¼Œç›´æ¥ç”¨ï¼
            if "rgb_features" in observations:
                features = observations["rgb_features"]
            else:
                # åªæœ‰æ²¡ç¼“å­˜çš„æ—¶å€™ï¼Œæ‰è·‘ CLIP Backbone
                rgb = observations["rgb"]
                
                # 1. æ•°æ®é¢„å¤„ç†
                # [B, H, W, 3] -> [B, 3, H, W] -> float -> 0-1
                rgb = rgb.permute(0, 3, 1, 2).float() / 255.0
                
                # åº”ç”¨ CLIP æ ‡å‡†åŒ–
                rgb = (rgb - self.clip_mean) / self.clip_std

                # 2. CLIP å‰å‘ä¼ æ’­
                outputs = self.backbone(pixel_values=rgb)
                features = outputs.last_hidden_state

            # 3. æå–ç‰¹å¾ (ä¸ç®¡æ˜¯ç¼“å­˜çš„è¿˜æ˜¯æ–°ç®—çš„ï¼Œæ ¼å¼éƒ½æ˜¯ [Batch, 50, 768])
            # å»æ‰ç¬¬ä¸€ä¸ª [CLS] token (ç´¢å¼•0)ï¼Œä¿ç•™åé¢çš„ spatial patches (ç´¢å¼•1-49)
            spatial_features = features[:, 1:, :]
            
            # 4. é™ç»´æŠ•å½± (è¿™ä¸ª Projection å±‚æ˜¯ä½ è¦è®­ç»ƒçš„ï¼)
            spatial_features = self.projection(spatial_features)

            # 5. è°ƒæ•´å½¢çŠ¶
            return spatial_features.permute(0, 2, 1)
    # def forward(self, observations):
    #         """
    #         observations["rgb"]: [Batch, H, W, 3] (uint8 0-255)
    #         """
    #         rgb = observations["rgb"]
            
    #         # 1. æ•°æ®é¢„å¤„ç†
    #         # [B, H, W, 3] -> [B, 3, H, W] -> float -> 0-1
    #         rgb = rgb.permute(0, 3, 1, 2).float() / 255.0
            
    #         # åº”ç”¨ CLIP æ ‡å‡†åŒ–
    #         # 2. æ‰‹åŠ¨å½’ä¸€åŒ– (ç›´æ¥ç”¨ Tensor è®¡ç®—ï¼Œæ”¯æŒ Batch)
    #         # åˆ æ‰: rgb = self.preprocess(rgb)
    #         rgb = (rgb - self.clip_mean) / self.clip_std

    #         # 2. CLIP å‰å‘ä¼ æ’­
    #         # è¾“å‡º: [Batch, Seq_Len(50), Hidden(768)]
    #         outputs = self.backbone(pixel_values=rgb)
    #         features = outputs.last_hidden_state

    #         # 3. æå–ç‰¹å¾
    #         # å»æ‰ç¬¬ä¸€ä¸ª [CLS] token (ç´¢å¼•0)ï¼Œä¿ç•™åé¢çš„ spatial patches (ç´¢å¼•1-49)
    #         # features å˜æˆ [Batch, 49, 768]
    #         spatial_features = features[:, 1:, :]
            
    #         # 4. é™ç»´æŠ•å½±
    #         # [Batch, 49, 768] -> [Batch, 49, 512]
    #         spatial_features = self.projection(spatial_features)

    #         # 5. è°ƒæ•´å½¢çŠ¶ä»¥é€‚é… CMANet
    #         # CMANet æœŸæœ›çš„æ˜¯ [Batch, Channel, Spatial/Seq]
    #         # æ‰€ä»¥æˆ‘ä»¬éœ€è¦è½¬ç½®: [Batch, 512, 49]
    #         return spatial_features.permute(0, 2, 1)
import gzip
import json

import torch
import torch.nn as nn
from habitat import Config
from habitat.core.simulator import Observations
from torch import Tensor
from transformers import BertModel

class InstructionEncoder(nn.Module):
    def __init__(self, config: Config) -> None:
        """An encoder that uses RNN to encode an instruction. Returns
        the final hidden state after processing the instruction sequence.

        Args:
            config: must have
                embedding_size: The dimension of each embedding vector
                hidden_size: The hidden (output) size
                rnn_type: The RNN cell type.  Must be GRU or LSTM
                final_state_only: If True, return just the final state
        """
        super().__init__()

        self.config = config

        rnn = nn.GRU if self.config.rnn_type == "GRU" else nn.LSTM
        self.encoder_rnn = rnn(
            input_size=config.embedding_size,
            hidden_size=config.hidden_size,
            bidirectional=config.bidirectional,
        )

        if config.sensor_uuid == "instruction":
            if self.config.use_pretrained_embeddings:
                self.embedding_layer = nn.Embedding.from_pretrained(
                    embeddings=self._load_embeddings(),
                    freeze=not self.config.fine_tune_embeddings,
                )
            else:  # each embedding initialized to sampled Gaussian
                self.embedding_layer = nn.Embedding(
                    num_embeddings=config.vocab_size,
                    embedding_dim=config.embedding_size,
                    padding_idx=0,
                )

    @property
    def output_size(self):
        return self.config.hidden_size * (1 + int(self.config.bidirectional))

    def _load_embeddings(self) -> Tensor:
        """Loads word embeddings from a pretrained embeddings file.
        PAD: index 0. [0.0, ... 0.0]
        UNK: index 1. mean of all R2R word embeddings: [mean_0, ..., mean_n]
        why UNK is averaged: https://bit.ly/3u3hkYg
        Returns:
            embeddings tensor of size [num_words x embedding_dim]
        """
        with gzip.open(self.config.embedding_file, "rt") as f:
            embeddings = torch.tensor(json.load(f))
        return embeddings

    def forward(self, observations: Observations) -> Tensor:
        """
        Tensor sizes after computation:
            instruction: [batch_size x seq_length]
            lengths: [batch_size]
            hidden_state: [batch_size x hidden_size]
        """
        if self.config.sensor_uuid == "instruction":
            instruction = observations["instruction"].long()


            # ------------------ âœ… æœ€ç»ˆä¿®å¤ä»£ç  ------------------
            # è·å–è¯è¡¨å¤§å° (2504)
            vocab_size = self.embedding_layer.num_embeddings
            
            # åˆ›å»ºè¶Šç•Œæ©ç 
            # å‡¡æ˜¯ >= 2504 çš„ ID éƒ½æ˜¯éæ³•çš„
            out_of_bounds = instruction >= vocab_size
            
            # å¦‚æœå‘ç°è¶Šç•Œï¼Œå°†å…¶æ›¿æ¢ä¸º 1 (UNK - Unknown Token)
            # è¿™æ ·æ¨¡å‹å°±ä¼šæŠŠå®ƒå½“æˆ"ä¸€ä¸ªä¸è®¤è¯†çš„è¯"æ¥å¤„ç†ï¼Œè€Œä¸ä¼šå´©æºƒ
            if out_of_bounds.any():
                # (å¯é€‰) å¦‚æœä½ æƒ³ä¿ç•™æ—¥å¿—ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œï¼Œåªæ‰“å°ä¸æŠ¥é”™
                print(f"âš ï¸ Warning: Clamping {out_of_bounds.sum()} tokens (>= {vocab_size}) to UNK.")
                instruction[out_of_bounds] = 1 
            # -----------------------------------------------------
            # # ------------------ ğŸ”¥ DEBUG ä»£ç å¼€å§‹ ğŸ”¥ ------------------
            # # è·å– Embedding å±‚çš„è¯è¡¨å¤§å°
            # vocab_size = self.embedding_layer.num_embeddings
            
            # # è·å–å½“å‰ Batch ä¸­æœ€å¤§çš„ Token ID
            # max_token_id = instruction.max().item()
            
            # # æ£€æŸ¥æ˜¯å¦è¶Šç•Œ
            # if max_token_id >= vocab_size:
            #     print(f"\n{'='*40}")
            #     print(f"ğŸ”¥ã€ä¸¥é‡é”™è¯¯ã€‘CUDA Device-side Assert Triggered é¢„è­¦")
            #     print(f"ğŸ”¥ æ£€æµ‹åˆ°è¶Šç•Œ Token ID: {max_token_id}")
            #     print(f"ğŸ”¥ å½“å‰ Embedding è¯è¡¨å¤§å°: {vocab_size}")
            #     print(f"ğŸ”¥ è¶Šç•Œä½ç½® (Batch Index, Seq Index): {torch.nonzero(instruction >= vocab_size, as_tuple=False)}")
            #     print(f"{'='*40}\n")
                
            #     # å¼ºåˆ¶æŠ¥é”™ï¼Œé˜»æ­¢ä»£ç ç»§ç»­è¿è¡Œå¯¼è‡´ CUDA å´©æºƒçœ‹ä¸åˆ°æ—¥å¿—
            #     raise ValueError(f"Found token {max_token_id} >= vocab size {vocab_size}")
            # # ------------------ ğŸ”¥ DEBUG ä»£ç ç»“æŸ ğŸ”¥ ------------------

            lengths = (instruction != 0.0).long().sum(dim=1)
            instruction = self.embedding_layer(instruction)
        else:
            instruction = observations["rxr_instruction"]

        lengths = (instruction != 0.0).long().sum(dim=2)
        lengths = (lengths != 0.0).long().sum(dim=1).cpu()

        packed_seq = nn.utils.rnn.pack_padded_sequence(
            instruction, lengths, batch_first=True, enforce_sorted=False
        )

        output, final_state = self.encoder_rnn(packed_seq)

        if self.config.rnn_type == "LSTM":
            final_state = final_state[0]

        if self.config.final_state_only:
            return final_state.squeeze(0)
        else:
            return nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[
                0
            ].permute(0, 2, 1)


class BertInstructionEncoder(nn.Module):
    def __init__(self, config: Config) -> None:
        """
        BERT Encoder with correct shape for CMAPolicy.
        """
        super().__init__()
        self.config = config

        print(f"Loading BERT Model (fine-tune={getattr(config, 'fine_tune_bert', True)})...")
        self.bert = BertModel.from_pretrained("bert-base-uncased")

        # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®åœ¨è¿™é‡Œï¼Encoder è‡ªå·±è´Ÿè´£è¯»å– fine_tune_bert ğŸ”¥ğŸ”¥ğŸ”¥
        # getattr(config, "key", Default) çš„æ„æ€æ˜¯ï¼š
        # å» config é‡Œæ‰¾ "fine_tune_bert"ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œé»˜è®¤è®¤ä¸ºæ˜¯ True
        self.fine_tune = getattr(config, "fine_tune_bert", True)

        # æ‰§è¡Œå†»ç»“é€»è¾‘
        if not self.fine_tune:
            print("ğŸ¥¶ BERT Parameters Frozen (Like CLIP visual encoder)")
            for param in self.bert.parameters():
                param.requires_grad = False
        else:
            print("ğŸ”¥ BERT Parameters Unfrozen (Fine-tuning enabled)")
        self.bert_dim = self.bert.config.hidden_size
        self.hidden_size = config.hidden_size
        
        self.projection = nn.Sequential(
            nn.Linear(self.bert_dim, self.hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        self.attn_fc = nn.Sequential(
            nn.Linear(self.bert_dim, 128),
            nn.Tanh(),
            nn.Linear(128, 1)
        )

    @property
    def output_size(self):
        return self.hidden_size

    def forward(self, observations: Observations) -> Tensor:
        """
        Input: observations["instruction"] must be BERT Token IDs
        """
        input_ids = observations["instruction"].long()

        # # ğŸ”¥ğŸ”¥ğŸ”¥ DEBUG START ğŸ”¥ğŸ”¥ğŸ”¥
        # # 1. æ‰“å°å½¢çŠ¶ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ä½ è®¾ç½®çš„å›ºå®šé•¿åº¦ï¼ˆä¾‹å¦‚ 120 æˆ– 128ï¼‰
        # #    å¦‚æœæ˜¯å˜é•¿ï¼ˆæ¯ä¸ªbatchä¸ä¸€æ ·ï¼‰ï¼Œè¯´æ˜è¿˜åœ¨ç”¨æ—§ Sensorï¼
        # print(f"ğŸ§ [Debug] Input Shape: {input_ids.shape}")

        # # 2. æ‰“å°æœ€å¤§ Token IDï¼š
        # #    æ—§ R2R è¯è¡¨æœ€å¤§ ID åªæœ‰ ~2500ã€‚
        # #    BERT è¯è¡¨æœ€å¤§ ID æ˜¯ 30522ã€‚
        # #    å¦‚æœä½ çœ‹åˆ° > 2500 çš„æ•°å­—ï¼Œè¯´æ˜è‚¯å®šæ˜¯ BERT Tokenizer ç”Ÿæ•ˆäº†ï¼
        # max_id = input_ids.max().item()
        # print(f"ğŸ§ [Debug] Max Token ID: {max_id}")
        
        # # 3. æ‰“å°å‰å‡ ä¸ª Tokenï¼šçœ‹çœ‹æ˜¯ä¸æ˜¯ BERT çš„ç‰¹å¾ï¼ˆæ¯”å¦‚ 101 å¼€å¤´ï¼‰
        # #    101 æ˜¯ BERT çš„ [CLS] æ ‡è®°ã€‚æ—§ R2R æ•°æ®é›†é€šå¸¸ä¸ä¼šä»¥ 101 å¼€å¤´ã€‚
        # print(f"ğŸ§ [Debug] First 5 tokens: {input_ids[0, :5].tolist()}")
        # # ğŸ”¥ğŸ”¥ğŸ”¥ DEBUG END ğŸ”¥ğŸ”¥ğŸ”¥

        attention_mask = (input_ids != 0).long()

        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state # [Batch, Seq, 768]

        # 1. æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦ [Batch, Seq, 256]
        out = self.projection(sequence_output)

        # ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®æ­£ç‚¹ 1: æ‰‹åŠ¨å°† Padding åŒºåŸŸæŠ¹é›¶
        # è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼å› ä¸º CMAPolicy é‡Œé¢é€šè¿‡ (emb == 0).all() æ¥åˆ¤æ–­å“ªäº›æ˜¯ padding
        # å¦‚æœä¸æŠ¹é›¶ï¼ŒMask å°±ä¼šå¤±æ•ˆï¼Œæ¨¡å‹ä¼šæŠŠ Padding ä¹Ÿå½“æˆæœ‰æ•ˆå•è¯å¤„ç†
        out = out * attention_mask.unsqueeze(-1).float()

        if self.config.final_state_only:
            # Attention Pooling é€»è¾‘...
            attn_scores = self.attn_fc(sequence_output)
            mask = attention_mask.unsqueeze(-1).float()
            attn_scores = attn_scores + (1.0 - mask) * -1e9
            attn_weights = torch.softmax(attn_scores, dim=1)
            final_embed = torch.sum(sequence_output * attn_weights, dim=1)
            return self.projection(final_embed)
        else:
            # ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®æ­£ç‚¹ 2: ç»´åº¦ç½®æ¢ (Permute)
            # CMAPolicy é‡Œçš„ Conv1d æœŸæœ›è¾“å…¥æ˜¯ [Batch, Channel, Length]
            # è€Œ BERT è¾“å‡ºæ˜¯ [Batch, Length, Channel]
            # æ‰€ä»¥æˆ‘ä»¬éœ€è¦æŠŠç»´åº¦ 1 å’Œ 2 æ¢ä¸€ä¸‹
            return out.permute(0, 2, 1)